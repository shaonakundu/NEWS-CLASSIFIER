{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('news-classifier': conda)",
   "display_name": "Python 3.7.9 64-bit ('news-classifier': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6ff3cbf680f28ca2453fbcd94fe6869e1250d737a3f8cbf428af57fe7a4c4788"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "import nltk\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found punkt tokenizer at C:\\Users\\User1\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\\PY3\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer_path = nltk.data.find('tokenizers/punkt')\n",
    "    print(\"Found punkt tokenizer at {}\".format(tokenizer_path))\n",
    "except LookupError:\n",
    "    print(\"Downloading tokenizer\")\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Train data and test data\n",
    "def downloadCSV(url,filename):\n",
    "    if checkExists(filename=filename):\n",
    "        exit()\n",
    "    else:\n",
    "        # Streaming, so we can iterate over the response.\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024 #1 Kibibyte\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        with open(filename, 'wb') as file:\n",
    "            for data in response.iter_content(block_size):\n",
    "                progress_bar.update(len(data))\n",
    "                file.write(data)\n",
    "        progress_bar.close()\n",
    "\n",
    "def checkExists(filename):\n",
    "    if filename in os.listdir(\"./\"):\n",
    "        print(\"{} already exists in path {}\".format(filename,os.getcwd()))\n",
    "        return True\n",
    "    else:\n",
    "        print(\"{} not found in current directory {}\".format(filename,os.getcwd()))\n",
    "        print(\"Downloading {}\".format(filename))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train.csv not found in current directory g:\\NEWS-CLASSIFIER\\training\n",
      "Downloading train.csv\n",
      "98.6MiB [06:15, 263kiB/s]\n",
      "test.csv not found in current directory g:\\NEWS-CLASSIFIER\\training\n",
      "Downloading test.csv\n",
      "25.1MiB [01:09, 362kiB/s]\n"
     ]
    }
   ],
   "source": [
    "train_csv_url = \"https://raw.githubusercontent.com/deepraj1729/Spam-classification-Text-dataset/newbranch/data/train.csv\"\n",
    "test_csv_url = \"https://raw.githubusercontent.com/deepraj1729/Spam-classification-Text-dataset/newbranch/data/test.csv\"\n",
    "\n",
    "downloadCSV(train_csv_url,'train.csv')\n",
    "downloadCSV(test_csv_url,'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "RAW_PATH = r\"train.csv\"\n",
    "DATA_PATH = os.path.join(\"./\",RAW_PATH)\n",
    "\n",
    "#Read from CSV\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2   2                  Why the Truth Might Get You Fired   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4   4  Iranian woman jailed for fictional unpublished...   \n",
       "5   5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
       "6   6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
       "7   7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
       "8   8  Excerpts From a Draft Script for Donald Trump’...   \n",
       "9   9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
       "\n",
       "                         author  \\\n",
       "0                 Darrell Lucus   \n",
       "1               Daniel J. Flynn   \n",
       "2            Consortiumnews.com   \n",
       "3               Jessica Purkiss   \n",
       "4                Howard Portnoy   \n",
       "5               Daniel Nussbaum   \n",
       "6                           NaN   \n",
       "7               Alissa J. Rubin   \n",
       "8                           NaN   \n",
       "9  Megan Twohey and Scott Shane   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "5  In these trying times, Jackie Mason is the Voi...      0  \n",
       "6  Ever wonder how Britain’s most iconic pop pian...      1  \n",
       "7  PARIS  —   France chose an idealistic, traditi...      0  \n",
       "8  Donald J. Trump is scheduled to make a highly ...      0  \n",
       "9  A week before Michael T. Flynn resigned as nat...      0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>author</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>Darrell Lucus</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n      <td>Daniel J. Flynn</td>\n      <td>Ever get the feeling your life circles the rou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Why the Truth Might Get You Fired</td>\n      <td>Consortiumnews.com</td>\n      <td>Why the Truth Might Get You Fired October 29, ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n      <td>Jessica Purkiss</td>\n      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Iranian woman jailed for fictional unpublished...</td>\n      <td>Howard Portnoy</td>\n      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Jackie Mason: Hollywood Would Love Trump if He...</td>\n      <td>Daniel Nussbaum</td>\n      <td>In these trying times, Jackie Mason is the Voi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Life: Life Of Luxury: Elton John’s 6 Favorite ...</td>\n      <td>NaN</td>\n      <td>Ever wonder how Britain’s most iconic pop pian...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Benoît Hamon Wins French Socialist Party’s Pre...</td>\n      <td>Alissa J. Rubin</td>\n      <td>PARIS  —   France chose an idealistic, traditi...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>Excerpts From a Draft Script for Donald Trump’...</td>\n      <td>NaN</td>\n      <td>Donald J. Trump is scheduled to make a highly ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>A Back-Channel Plan for Ukraine and Russia, Co...</td>\n      <td>Megan Twohey and Scott Shane</td>\n      <td>A week before Michael T. Flynn resigned as nat...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Let's see the shape of our dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20800 entries, 0 to 20799\nData columns (total 5 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   id      20800 non-null  int64 \n 1   title   20242 non-null  object\n 2   author  18843 non-null  object\n 3   text    20761 non-null  object\n 4   label   20800 non-null  int64 \ndtypes: int64(2), object(3)\nmemory usage: 812.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Info of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# checking for NULL values for each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input column - \"text\"\n",
    "# Output Column - \"label\"\n",
    "# \"text\" column has 39 missing data rows\n",
    "# but you see total 20800 text rows, so, dropping these 39 rows will do no effect\n",
    "\n",
    "df.dropna(subset = [\"text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20761, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Counter({0: 10387, 1: 10374})\n"
     ]
    }
   ],
   "source": [
    "#checking Class distribution\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(df['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fairly Balanced ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input- X Output- Y\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_X):\n",
    "    # Replace email addresses with 'emailaddr'\n",
    "    X = data_X.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','emailaddr')\n",
    "\n",
    "    # Replace URLs with 'webaddr'\n",
    "    X = X.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddr')\n",
    "\n",
    "    # Replace Currency symbols with 'currsymb' \n",
    "    X = X.str.replace(r'£|\\$|₹', 'currsymb')\n",
    "\n",
    "    # Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenbr'\n",
    "    X = X.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenbr')\n",
    "\n",
    "    # Replace numbers with 'numbr'\n",
    "    X = X.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
    "\n",
    "    # Remove punctuation\n",
    "    X = X.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "    # Replace whitespace between terms with a single space\n",
    "    X = X.str.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Remove leading and trailing whitespace\n",
    "    X = X.str.replace(r'^\\s+|\\s+?$', '')\n",
    "\n",
    "    # To lowercase\n",
    "    X = X.str.lower()\n",
    "\n",
    "    \n",
    "\n",
    "    # Stemming words (removing ing, ed ...)\n",
    "    # ps = nltk.PorterStemmer()\n",
    "    # X = X.apply(lambda x: ' '.join(ps.stem(term) for term in x.split()))\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "# Call the Preprocess function\n",
    "X_preprocessed = preprocess(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a pipeline for removing stopwords and create bag of words\n",
    "# applying multinomialNB as it gives us better results\n",
    "\n",
    "model = Pipeline([('tfidf', TfidfVectorizer(stop_words='english')), ('nbmodel', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline(memory=None,\n         steps=[('tfidf',\n                 TfidfVectorizer(analyzer='word', binary=False,\n                                 decode_error='strict',\n                                 dtype=<class 'numpy.float64'>,\n                                 encoding='utf-8', input='content',\n                                 lowercase=True, max_df=1.0, max_features=None,\n                                 min_df=1, ngram_range=(1, 1), norm='l2',\n                                 preprocessor=None, smooth_idf=True,\n                                 stop_words='english', strip_accents=None,\n                                 sublinear_tf=False,\n                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                 tokenizer=None, use_idf=True,\n                                 vocabulary=None)),\n                ('nbmodel',\n                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n         verbose=False)"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "precision    recall  f1-score   support\n\n           0       0.78      0.99      0.87      2579\n           1       0.98      0.72      0.83      2612\n\n    accuracy                           0.85      5191\n   macro avg       0.88      0.85      0.85      5191\nweighted avg       0.88      0.85      0.85      5191\n\n"
    }
   ],
   "source": [
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[2550   29]\n [ 731 1881]]\n"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that the Fasle Positive (36) and False Negative (664) are a bit high\n",
    "# Some other approaches may help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the pickle file\n",
    "with open('../saved_model/model1.pickle', 'wb') as target:\n",
    "    pickle.dump(model, target, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ]
}